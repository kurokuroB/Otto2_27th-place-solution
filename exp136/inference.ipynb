{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import cudf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "from catboost import CatBoost, CatBoostRanker\n",
    "from catboost import Pool\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "print(os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グローバル変数設定\n",
    "N_SPLITS = 4\n",
    "CHUNKS = 12  # 推論分割数\n",
    "\n",
    "ROOT = \"\"  # コンペ用ディレクトリ\n",
    "OUTPUT_DIR = \"\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリ削減（数値カラムのみ）\n",
    "def reduce_mem_usage_for_numeric(df):\n",
    "    \"\"\"iterate through  the numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if \"int\" in str(col_type) or \"float\" in str(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if \"int\" in str(col_type):\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            elif \"float\" in str(col_type):\n",
    "                # if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                #     df[col] = df[col].astype(np.float16)# サポート対象故\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(f\"{ROOT}/data/input/processed_data2/test.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = pd.read_pickle(f\"/{OUTPUT_DIR}/test_click_candidates.pkl\")\n",
    "all_candidates = pl.from_pandas(all_candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/TrimBaseItemFeatures.pkl\")\n",
    "item_features = item_features.reset_index()\n",
    "item_features = pl.from_pandas(item_features)\n",
    "\n",
    "user_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseUserFeatures.pkl\")\n",
    "user_features = user_features.reset_index()\n",
    "user_features = pl.from_pandas(user_features)\n",
    "\n",
    "user_item_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseInteractiveFeatures.pkl\")\n",
    "user_item_features = user_item_features.reset_index()\n",
    "user_item_features = pl.from_pandas(user_item_features)\n",
    "\n",
    "item_count_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/TrimItemCountFeatures.pkl\")\n",
    "item_count_features = item_count_features.reset_index()\n",
    "item_count_features = pl.from_pandas(item_count_features)\n",
    "\n",
    "popularity_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/PopularityFeatures.pkl\")\n",
    "popularity_features = pl.from_pandas(popularity_features)\n",
    "\n",
    "item_count_features2 = pd.read_pickle(f\"{ROOT}/data/output/features/test/ItemCountFeatures2.pkl\")\n",
    "item_count_features2 = item_count_features2.reset_index()\n",
    "item_count_features2 = pl.from_pandas(item_count_features2)\n",
    "\n",
    "item_features2 = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseItemFeatures2.pkl\")\n",
    "item_features2 = item_features2.reset_index()\n",
    "item_features2 = pl.from_pandas(item_features2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_cols(df, columns):\n",
    "    for col in columns:\n",
    "        df = df.with_column(df.get_column(col).cast(pl.Int32))\n",
    "    return df\n",
    "\n",
    "\n",
    "item_features = cast_cols(item_features, [\"aid\"])\n",
    "user_features = cast_cols(user_features, [\"session\"])\n",
    "user_item_features = cast_cols(user_item_features, [\"session\", \"aid\"])\n",
    "item_count_features = cast_cols(item_count_features, [\"aid\"])\n",
    "popularity_features = cast_cols(popularity_features, [\"aid\"])\n",
    "item_count_features2 = cast_cols(item_count_features2, [\"aid\"])\n",
    "item_features2 = cast_cols(item_features2, [\"aid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = math.ceil(len(all_candidates) / CHUNKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_prediction = np.zeros(len(all_candidates))\n",
    "for chunk in range(CHUNKS):\n",
    "    print(\"chunk\", chunk)\n",
    "    # ここでchunk分を取り出し、特徴量を結合する\n",
    "    target_candidates = all_candidates[chunk * chunk_len : (chunk + 1) * chunk_len]\n",
    "\n",
    "    target_candidates = target_candidates.join(item_features, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(user_features, on=\"session\", how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(user_item_features, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(item_count_features, on=\"aid\", how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(popularity_features, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(item_count_features2, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(item_features2, on=\"aid\", how=\"left\")\n",
    "\n",
    "    # pandasにもどす\n",
    "    target_candidates = target_candidates.to_pandas()\n",
    "    with open(f\"{OUTPUT_DIR}/click_features.pkl\", \"rb\") as f:\n",
    "        FEATURES = pickle.load(f)\n",
    "\n",
    "    for fold in range(N_SPLITS):\n",
    "        print(\"fold\", fold)\n",
    "\n",
    "        with open(f\"{OUTPUT_DIR}/fold{fold}_click_cbt.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        prediction = model.predict(target_candidates[FEATURES])\n",
    "\n",
    "        folds_prediction[chunk * chunk_len : (chunk + 1) * chunk_len] += prediction\n",
    "\n",
    "    folds_prediction[chunk * chunk_len : (chunk + 1) * chunk_len] /= N_SPLITS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    item_features,\n",
    "    user_features,\n",
    "    user_item_features,\n",
    "    item_count_features,\n",
    "    popularity_features,\n",
    "    item_count_features2,\n",
    "    item_features2,\n",
    ")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = all_candidates.to_pandas()\n",
    "all_candidates[\"prediction\"] = folds_prediction\n",
    "all_candidates = all_candidates[[\"session\", \"aid\", \"prediction\"]]\n",
    "\n",
    "del folds_prediction\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortして上位20取り出す。\n",
    "all_candidates = all_candidates.sort_values(\"prediction\", ascending=False)\n",
    "all_candidates = all_candidates.groupby(\"session\").head(20)\n",
    "\n",
    "# 整形\n",
    "all_candidates = all_candidates.groupby(\"session\").aid.apply(list)\n",
    "\n",
    "all_candidates = all_candidates.reset_index()\n",
    "all_candidates[\"session\"] = all_candidates[\"session\"].apply(lambda x: str(x) + \"_clicks\")\n",
    "all_candidates[\"aid\"] = all_candidates[\"aid\"].apply(lambda x: \" \".join(list(map(str, x))))\n",
    "\n",
    "# カラム名変更\n",
    "all_candidates = all_candidates.rename(columns={\"session\": \"session_type\", \"aid\": \"labels\"})\n",
    "\n",
    "all_candidates.to_pickle(f\"{OUTPUT_DIR}/test_click_top20_candidates.pkl\")\n",
    "\n",
    "del all_candidates\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = pd.read_pickle(f\"/{OUTPUT_DIR}/test_cart_order_candidates.pkl\")\n",
    "all_candidates = pl.from_pandas(all_candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/TrimBaseItemFeatures.pkl\")\n",
    "item_features = item_features.reset_index()\n",
    "item_features = pl.from_pandas(item_features)\n",
    "\n",
    "user_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseUserFeatures.pkl\")\n",
    "user_features = user_features.reset_index()\n",
    "user_features = pl.from_pandas(user_features)\n",
    "\n",
    "user_item_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseInteractiveFeatures.pkl\")\n",
    "user_item_features = user_item_features.reset_index()\n",
    "user_item_features = pl.from_pandas(user_item_features)\n",
    "\n",
    "item_count_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/TrimItemCountFeatures.pkl\")\n",
    "item_count_features = item_count_features.reset_index()\n",
    "item_count_features = pl.from_pandas(item_count_features)\n",
    "\n",
    "popularity_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/PopularityFeatures.pkl\")\n",
    "popularity_features = pl.from_pandas(popularity_features)\n",
    "\n",
    "item_count_features2 = pd.read_pickle(f\"{ROOT}/data/output/features/test/ItemCountFeatures2.pkl\")\n",
    "item_count_features2 = item_count_features2.reset_index()\n",
    "item_count_features2 = pl.from_pandas(item_count_features2)\n",
    "\n",
    "item_features2 = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseItemFeatures2.pkl\")\n",
    "item_features2 = item_features2.reset_index()\n",
    "item_features2 = pl.from_pandas(item_features2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_cols(df, columns):\n",
    "    for col in columns:\n",
    "        df = df.with_column(df.get_column(col).cast(pl.Int32))\n",
    "    return df\n",
    "\n",
    "\n",
    "item_features = cast_cols(item_features, [\"aid\"])\n",
    "user_features = cast_cols(user_features, [\"session\"])\n",
    "user_item_features = cast_cols(user_item_features, [\"session\", \"aid\"])\n",
    "item_count_features = cast_cols(item_count_features, [\"aid\"])\n",
    "popularity_features = cast_cols(popularity_features, [\"aid\"])\n",
    "item_count_features2 = cast_cols(item_count_features2, [\"aid\"])\n",
    "item_features2 = cast_cols(item_features2, [\"aid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = math.ceil(len(all_candidates) / CHUNKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_prediction = np.zeros(len(all_candidates))\n",
    "for chunk in range(CHUNKS):\n",
    "    print(\"chunk\", chunk)\n",
    "    # ここでchunk分を取り出し、特徴量を結合する\n",
    "    target_candidates = all_candidates[chunk * chunk_len : (chunk + 1) * chunk_len]\n",
    "\n",
    "    target_candidates = target_candidates.join(item_features, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(user_features, on=\"session\", how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(user_item_features, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(item_count_features, on=\"aid\", how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(popularity_features, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(item_count_features2, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(item_features2, on=\"aid\", how=\"left\")\n",
    "\n",
    "    # pandasにもどす\n",
    "    target_candidates = target_candidates.to_pandas()\n",
    "    with open(f\"{OUTPUT_DIR}/cart_features.pkl\", \"rb\") as f:\n",
    "        FEATURES = pickle.load(f)\n",
    "\n",
    "    for fold in range(N_SPLITS):\n",
    "        print(\"fold\", fold)\n",
    "\n",
    "        with open(f\"{OUTPUT_DIR}/fold{fold}_cart_cbt.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        prediction = model.predict(target_candidates[FEATURES])\n",
    "\n",
    "        folds_prediction[chunk * chunk_len : (chunk + 1) * chunk_len] += prediction\n",
    "\n",
    "    folds_prediction[chunk * chunk_len : (chunk + 1) * chunk_len] /= N_SPLITS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    item_features,\n",
    "    user_features,\n",
    "    user_item_features,\n",
    "    item_count_features,\n",
    "    popularity_features,\n",
    "    item_count_features2,\n",
    "    item_features2,\n",
    ")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = all_candidates.to_pandas()\n",
    "all_candidates[\"prediction\"] = folds_prediction\n",
    "all_candidates = all_candidates[[\"session\", \"aid\", \"prediction\"]]\n",
    "\n",
    "del folds_prediction\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortして上位20取り出す。\n",
    "all_candidates = all_candidates.sort_values(\"prediction\", ascending=False)\n",
    "all_candidates = all_candidates.groupby(\"session\").head(20)\n",
    "\n",
    "# 整形\n",
    "all_candidates = all_candidates.groupby(\"session\").aid.apply(list)\n",
    "\n",
    "all_candidates = all_candidates.reset_index()\n",
    "all_candidates[\"session\"] = all_candidates[\"session\"].apply(lambda x: str(x) + \"_carts\")\n",
    "all_candidates[\"aid\"] = all_candidates[\"aid\"].apply(lambda x: \" \".join(list(map(str, x))))\n",
    "\n",
    "# カラム名変更\n",
    "all_candidates = all_candidates.rename(columns={\"session\": \"session_type\", \"aid\": \"labels\"})\n",
    "\n",
    "all_candidates.to_pickle(f\"{OUTPUT_DIR}/test_cart_top20_candidates.pkl\")\n",
    "\n",
    "del all_candidates\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = pd.read_pickle(f\"/{OUTPUT_DIR}/test_cart_order_candidates.pkl\")\n",
    "all_candidates = pl.from_pandas(all_candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/TrimBaseItemFeatures.pkl\")\n",
    "item_features = item_features.reset_index()\n",
    "item_features = pl.from_pandas(item_features)\n",
    "\n",
    "user_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseUserFeatures.pkl\")\n",
    "user_features = user_features.reset_index()\n",
    "user_features = pl.from_pandas(user_features)\n",
    "\n",
    "user_item_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseInteractiveFeatures.pkl\")\n",
    "user_item_features = user_item_features.reset_index()\n",
    "user_item_features = pl.from_pandas(user_item_features)\n",
    "\n",
    "item_count_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/TrimItemCountFeatures.pkl\")\n",
    "item_count_features = item_count_features.reset_index()\n",
    "item_count_features = pl.from_pandas(item_count_features)\n",
    "\n",
    "popularity_features = pd.read_pickle(f\"{ROOT}/data/output/features/test/PopularityFeatures.pkl\")\n",
    "popularity_features = pl.from_pandas(popularity_features)\n",
    "\n",
    "item_count_features2 = pd.read_pickle(f\"{ROOT}/data/output/features/test/ItemCountFeatures2.pkl\")\n",
    "item_count_features2 = item_count_features2.reset_index()\n",
    "item_count_features2 = pl.from_pandas(item_count_features2)\n",
    "\n",
    "item_features2 = pd.read_pickle(f\"{ROOT}/data/output/features/test/BaseItemFeatures2.pkl\")\n",
    "item_features2 = item_features2.reset_index()\n",
    "item_features2 = pl.from_pandas(item_features2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_cols(df, columns):\n",
    "    for col in columns:\n",
    "        df = df.with_column(df.get_column(col).cast(pl.Int32))\n",
    "    return df\n",
    "\n",
    "\n",
    "item_features = cast_cols(item_features, [\"aid\"])\n",
    "user_features = cast_cols(user_features, [\"session\"])\n",
    "user_item_features = cast_cols(user_item_features, [\"session\", \"aid\"])\n",
    "item_count_features = cast_cols(item_count_features, [\"aid\"])\n",
    "popularity_features = cast_cols(popularity_features, [\"aid\"])\n",
    "item_count_features2 = cast_cols(item_count_features2, [\"aid\"])\n",
    "item_features2 = cast_cols(item_features2, [\"aid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = math.ceil(len(all_candidates) / CHUNKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_prediction = np.zeros(len(all_candidates))\n",
    "for chunk in range(CHUNKS):\n",
    "    print(\"chunk\", chunk)\n",
    "    # ここでchunk分を取り出し、特徴量を結合する\n",
    "    target_candidates = all_candidates[chunk * chunk_len : (chunk + 1) * chunk_len]\n",
    "\n",
    "    target_candidates = target_candidates.join(item_features, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(user_features, on=\"session\", how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(user_item_features, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(item_count_features, on=\"aid\", how=\"left\")\n",
    "\n",
    "    target_candidates = target_candidates.join(popularity_features, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(item_count_features2, on=\"aid\", how=\"left\")\n",
    "    target_candidates = target_candidates.join(item_features2, on=\"aid\", how=\"left\")\n",
    "\n",
    "    # pandasにもどす\n",
    "    target_candidates = target_candidates.to_pandas()\n",
    "\n",
    "    with open(f\"{OUTPUT_DIR}/order_features.pkl\", \"rb\") as f:\n",
    "        FEATURES = pickle.load(f)\n",
    "\n",
    "    for fold in range(N_SPLITS):\n",
    "        print(\"fold\", fold)\n",
    "\n",
    "        with open(f\"{OUTPUT_DIR}/fold{fold}_order_cbt.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        prediction = model.predict(target_candidates[FEATURES])\n",
    "\n",
    "        folds_prediction[chunk * chunk_len : (chunk + 1) * chunk_len] += prediction\n",
    "\n",
    "    folds_prediction[chunk * chunk_len : (chunk + 1) * chunk_len] /= N_SPLITS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    item_features,\n",
    "    user_features,\n",
    "    user_item_features,\n",
    "    item_count_features,\n",
    "    popularity_features,\n",
    "    item_count_features2,\n",
    "    item_features2,\n",
    ")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = all_candidates.to_pandas()  # pandasに戻す\n",
    "all_candidates[\"prediction\"] = folds_prediction\n",
    "all_candidates = all_candidates[[\"session\", \"aid\", \"prediction\"]]\n",
    "\n",
    "del folds_prediction\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortして上位20取り出す。\n",
    "all_candidates = all_candidates.sort_values(\"prediction\", ascending=False)\n",
    "all_candidates = all_candidates.groupby(\"session\").head(20)\n",
    "\n",
    "# 整形\n",
    "all_candidates = all_candidates.groupby(\"session\").aid.apply(list)\n",
    "\n",
    "all_candidates = all_candidates.reset_index()\n",
    "all_candidates[\"session\"] = all_candidates[\"session\"].apply(lambda x: str(x) + \"_orders\")\n",
    "all_candidates[\"aid\"] = all_candidates[\"aid\"].apply(lambda x: \" \".join(list(map(str, x))))\n",
    "\n",
    "# カラム名変更\n",
    "all_candidates = all_candidates.rename(columns={\"session\": \"session_type\", \"aid\": \"labels\"})\n",
    "\n",
    "all_candidates.to_pickle(f\"{OUTPUT_DIR}/test_order_top20_candidates.pkl\")\n",
    "\n",
    "del all_candidates\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_candidates = pd.read_pickle(f\"{OUTPUT_DIR}/test_click_top20_candidates.pkl\")\n",
    "cart_candidates = pd.read_pickle(f\"{OUTPUT_DIR}/test_cart_top20_candidates.pkl\")\n",
    "order_candidates = pd.read_pickle(f\"{OUTPUT_DIR}/test_order_top20_candidates.pkl\")\n",
    "\n",
    "sub = pd.concat([click_candidates, cart_candidates, order_candidates], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f\"{OUTPUT_DIR}/submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rapids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
