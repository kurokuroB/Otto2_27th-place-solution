{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from catboost import CatBoost\n",
    "from catboost import Pool\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グローバル変数設定\n",
    "\n",
    "ROOT = \"\"  # コンペ用ディレクトリ\n",
    "OUTPUT_DIR = \"\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリ削減（数値カラムのみ）\n",
    "def reduce_mem_usage_for_numeric(df):\n",
    "    \"\"\"iterate through  the numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if \"int\" in str(col_type) or \"float\" in str(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if \"int\" in str(col_type):\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            elif \"float\" in str(col_type):\n",
    "                # if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                #     df[col] = df[col].astype(np.float16)# サポート対象故\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 理想時のcvを見ることができる。\n",
    "def calc_candidates_recall(candidates, label, type):\n",
    "    pred = candidates.groupby(\"session\").aid.apply(set)\n",
    "    pred = pred.reset_index()\n",
    "    gt = label[label[\"type\"] == type]\n",
    "\n",
    "    gt_pred = gt.merge(pred, on=\"session\", how=\"left\")\n",
    "\n",
    "    # negasamp後に、gtに紐づかないaidが出る可能性があるので、空のsetでfillna\n",
    "    gt_pred[\"aid\"] = gt_pred[\"aid\"].apply(lambda d: d if isinstance(d, set) else set())\n",
    "\n",
    "    gt_pred[\"hits\"] = gt_pred.apply(\n",
    "        lambda x: min(len(set(x[\"ground_truth\"]) & x[\"aid\"]), 20), axis=1\n",
    "    )\n",
    "    gt_pred[\"gt_count\"] = gt_pred.apply(\n",
    "        lambda x: min(len(x[\"ground_truth\"]), 20), axis=1\n",
    "    )\n",
    "    print(gt_pred.hits.sum() / gt_pred.gt_count.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_label = pd.read_pickle(f\"{ROOT}/data/input/valid_label_1week.pkl\")\n",
    "valid_session = pd.read_pickle(f\"{ROOT}/data/input/valid_trimed_session_1week.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_candidate.ipynb で作成した candidates を load & 目的変数生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_candidates = pd.read_pickle(f\"{OUTPUT_DIR}/valid_cart_order_candidates.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = valid_label[valid_label[\"type\"] == \"orders\"]\n",
    "gt = gt.explode(\"ground_truth\").reset_index(drop=True)\n",
    "\n",
    "CHUNKS = 4\n",
    "chunk_len = math.ceil(len(valid_candidates) / CHUNKS)\n",
    "\n",
    "chunk_candidates = []\n",
    "for chunk in range(CHUNKS):\n",
    "    print(\"chunk\", chunk)\n",
    "    target_candidates = valid_candidates.iloc[\n",
    "        chunk * chunk_len : (chunk + 1) * chunk_len\n",
    "    ]\n",
    "\n",
    "    target_candidates = target_candidates.merge(\n",
    "        gt, left_on=[\"session\", \"aid\"], right_on=[\"session\", \"ground_truth\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    target_candidates[\"target\"] = 1\n",
    "    target_candidates.loc[target_candidates[\"ground_truth\"].isnull(), \"target\"] = 0\n",
    "\n",
    "    target_candidates = target_candidates.drop([\"type\", \"ground_truth\"], axis=1)\n",
    "\n",
    "    # negative sample\n",
    "    positives = target_candidates.loc[target_candidates[\"target\"] == 1]\n",
    "    negatives = target_candidates.loc[target_candidates[\"target\"] == 0].sample(\n",
    "        frac=0.1, random_state=42\n",
    "    )\n",
    "    target_candidates = pd.concat([positives, negatives], axis=0, ignore_index=True)\n",
    "\n",
    "    chunk_candidates.append(target_candidates)\n",
    "\n",
    "del valid_candidates, target_candidates\n",
    "gc.collect()\n",
    "\n",
    "valid_candidates = pd.concat(chunk_candidates, ignore_index=True)\n",
    "del chunk_candidates\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_candidates.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 並びをランダム化する\n",
    "valid_candidates = valid_candidates.sample(frac=1, random_state=42)\n",
    "valid_candidates = valid_candidates.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量結合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseItemFeatures.pkl\"\n",
    ")\n",
    "user_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseUserFeatures.pkl\"\n",
    ")\n",
    "user_item_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseInteractiveFeatures.pkl\"\n",
    ")\n",
    "\n",
    "item_count_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/ItemCountFeatures.pkl\"\n",
    ")\n",
    "\n",
    "# popularity\n",
    "popularity_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/PopularityFeatures.pkl\"\n",
    ")\n",
    "\n",
    "# type count valid only\n",
    "item_count_features2 = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/ItemCountFeatures2.pkl\"\n",
    ")\n",
    "\n",
    "# base valid only\n",
    "item_features2 = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseItemFeatures2.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_candidates = valid_candidates.merge(item_features, on=\"aid\", how=\"left\")\n",
    "valid_candidates = valid_candidates.merge(user_features, on=\"session\", how=\"left\")\n",
    "\n",
    "valid_candidates = valid_candidates.merge(\n",
    "    user_item_features, on=[\"session\", \"aid\"], how=\"left\"\n",
    ")\n",
    "\n",
    "valid_candidates = valid_candidates.merge(item_count_features, on=\"aid\", how=\"left\")\n",
    "\n",
    "valid_candidates = valid_candidates.merge(popularity_features, on=\"aid\", how=\"left\")\n",
    "valid_candidates = valid_candidates.merge(item_count_features2, on=\"aid\", how=\"left\")\n",
    "\n",
    "valid_candidates = valid_candidates.merge(item_features2, on=\"aid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# castする\n",
    "valid_candidates = reduce_mem_usage_for_numeric(valid_candidates)\n",
    "\n",
    "# aidがobjectになることがあるのでintに\n",
    "valid_candidates.aid = valid_candidates.aid.astype(np.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"loss_function\": \"YetiRank\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"iterations\": 30000,\n",
    "    \"use_best_model\": True,\n",
    "    \"early_stopping_rounds\": 1000,\n",
    "    \"task_type\": \"GPU\",\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    col for col in valid_candidates.columns if col not in [\"session\", \"aid\", \"target\"]\n",
    "]\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/order_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(FEATURES, f)\n",
    "\n",
    "TARGET = \"target\"\n",
    "\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# rankerを使うからsort\n",
    "valid_candidates = valid_candidates.sort_values(\"session\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_index, valid_index) in enumerate(\n",
    "    sgkf.split(\n",
    "        valid_candidates, valid_candidates[\"target\"], groups=valid_candidates[\"session\"]\n",
    "    )\n",
    "):\n",
    "    train = valid_candidates.loc[train_index]\n",
    "    valid = valid_candidates.loc[valid_index]\n",
    "\n",
    "    train_X = train[FEATURES]\n",
    "    train_y = train[TARGET]\n",
    "\n",
    "    valid_X = valid[FEATURES]\n",
    "    valid_y = valid[TARGET]\n",
    "\n",
    "    train_pool = Pool(\n",
    "        data=train_X, label=train_y, group_id=train.session, cat_features=[]\n",
    "    )\n",
    "    eval_pool = Pool(\n",
    "        data=valid_X, label=valid_y, group_id=valid.session, cat_features=[]\n",
    "    )\n",
    "\n",
    "    model = CatBoost(params)\n",
    "    model.fit(train_pool, eval_set=eval_pool)\n",
    "\n",
    "    # 予測値を保存\n",
    "    val_pred = model.predict(valid_X)\n",
    "    valid[\"prediction\"] = val_pred\n",
    "\n",
    "    valid[[\"session\", \"aid\", \"target\", \"prediction\"]].to_csv(\n",
    "        f\"{OUTPUT_DIR}/fold{fold}_order_valid_prediction.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # モデル保存\n",
    "    with open(f\"{OUTPUT_DIR}/fold{fold}_order_cbt.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # メモリ解放\n",
    "    del val_pred\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## negative sample 後の cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall\n",
    "tmps = []\n",
    "for fold in range(4):\n",
    "    tmp = pd.read_csv(f\"{OUTPUT_DIR}/fold{fold}_order_valid_prediction.csv\")\n",
    "    tmps.append(tmp)\n",
    "\n",
    "pred = pd.concat(tmps).reset_index(drop=True)\n",
    "\n",
    "pred = pred.sort_values(\"prediction\", ascending=False)\n",
    "pred = pred.groupby(\"session\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"orders\"]:\n",
    "    print(f\"{type}_candidates_recall\")\n",
    "    calc_candidates_recall(pred, valid_label, type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## negative sample 前の cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_candidates = pd.read_pickle(f\"{OUTPUT_DIR}/valid_cart_order_candidates.pkl\")\n",
    "valid_candidates = pl.from_pandas(valid_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseItemFeatures.pkl\"\n",
    ")\n",
    "user_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseUserFeatures.pkl\"\n",
    ")\n",
    "user_item_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseInteractiveFeatures.pkl\"\n",
    ")\n",
    "\n",
    "item_count_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/ItemCountFeatures.pkl\"\n",
    ")\n",
    "\n",
    "# popularity\n",
    "popularity_features = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/PopularityFeatures.pkl\"\n",
    ")\n",
    "\n",
    "# type count valid only\n",
    "item_count_features2 = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/ItemCountFeatures2.pkl\"\n",
    ")\n",
    "\n",
    "# base valid only\n",
    "item_features2 = pd.read_pickle(\n",
    "    f\"{ROOT}/data/output/features/train/BaseItemFeatures2.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS = 16  # foldの推論を分割する\n",
    "\n",
    "all_candidates = []\n",
    "for fold in range(4):\n",
    "    print(fold)\n",
    "    tmp = pd.read_csv(f\"{OUTPUT_DIR}/fold{fold}_order_valid_prediction.csv\")\n",
    "    tmp = pl.from_pandas(tmp)\n",
    "    target_session = tmp[\"session\"].to_list()\n",
    "\n",
    "    # target sessionに限定\n",
    "    target_candidates = valid_candidates.filter(pl.col(\"session\").is_in(target_session))\n",
    "    target_candidates = target_candidates.to_pandas()\n",
    "\n",
    "    del tmp, target_session\n",
    "    gc.collect()\n",
    "\n",
    "    chunk_len = math.ceil(len(target_candidates) / CHUNKS)\n",
    "    for chunk in range(CHUNKS):\n",
    "        print(\"chunk\", chunk)\n",
    "\n",
    "        # ここでchunk分を取り出し、特徴量を結合する\n",
    "        chunk_candidates = target_candidates.iloc[\n",
    "            chunk * chunk_len : (chunk + 1) * chunk_len\n",
    "        ]\n",
    "\n",
    "        # 特徴量結合\n",
    "        chunk_candidates = chunk_candidates.merge(item_features, on=\"aid\", how=\"left\")\n",
    "        chunk_candidates = chunk_candidates.merge(\n",
    "            user_features, on=\"session\", how=\"left\"\n",
    "        )\n",
    "        chunk_candidates = chunk_candidates.merge(\n",
    "            user_item_features, on=[\"session\", \"aid\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        chunk_candidates = chunk_candidates.merge(\n",
    "            item_count_features, on=\"aid\", how=\"left\"\n",
    "        )\n",
    "\n",
    "        chunk_candidates = chunk_candidates.merge(\n",
    "            popularity_features, on=\"aid\", how=\"left\"\n",
    "        )\n",
    "        chunk_candidates = chunk_candidates.merge(\n",
    "            item_count_features2, on=\"aid\", how=\"left\"\n",
    "        )\n",
    "        chunk_candidates = chunk_candidates.merge(item_features2, on=\"aid\", how=\"left\")\n",
    "\n",
    "        # castする\n",
    "        chunk_candidates = reduce_mem_usage_for_numeric(chunk_candidates)\n",
    "\n",
    "        # 前準備\n",
    "        with open(f\"{OUTPUT_DIR}/order_features.pkl\", \"rb\") as f:\n",
    "            FEATURES = pickle.load(f)\n",
    "\n",
    "        with open(f\"{OUTPUT_DIR}/fold{fold}_order_cbt.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        chunk_candidates[\"prediction\"] = model.predict(chunk_candidates[FEATURES])\n",
    "\n",
    "        all_candidates.append(chunk_candidates[[\"session\", \"aid\", \"prediction\"]])\n",
    "\n",
    "        del chunk_candidates, model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del valid_candidates\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = pd.concat(all_candidates, ignore_index=True)\n",
    "# 各session上位20の予測に絞る\n",
    "all_candidates = all_candidates.sort_values(\"prediction\", ascending=False)\n",
    "all_candidates_top20 = all_candidates.groupby(\"session\").head(20)\n",
    "\n",
    "calc_candidates_recall(all_candidates_top20, valid_label, \"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存する\n",
    "all_candidates.to_pickle(f\"{OUTPUT_DIR}/final_order_valid_prediction.pkl\")\n",
    "all_candidates_top20.to_pickle(f\"{OUTPUT_DIR}/max20_final_order_valid_prediction.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rapids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
